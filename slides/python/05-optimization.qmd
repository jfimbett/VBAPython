---
title: "Python Session 5 — Numerical Optimization"
subtitle: "Root finding, minimization, and solving equations"
author: "Juan F. Imbet"
institute: "Paris Dauphine University-PSL"
format:
  revealjs:
    theme: white
    css: styles.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Intro to VBA and Python"
    include-in-header: ../../tikzjax.html
execute:
  enabled: false
---

## Why optimize?

- Find best parameters or satisfy equations


---

## SciPy optimize

```python
from scipy import optimize as opt
```

---

## Root finding: scalar

```python
import math
from scipy import optimize as opt

f = lambda x: x*math.cos(x) - 1
root = opt.brentq(f, 0, 2)
```

---

## Root finding: systems

```python
import numpy as np

def F(v):
    x, y = v
    return [x**2 + y**2 - 1, x - y]
sol = opt.root(F, [0.5, 0.1])
```

---

## Designing objectives

- Smooth and well-scaled objectives converge faster
- Prefer returning scalar; avoid non-differentiable kinks if possible

---

## Minimization: unconstrained

```python
quad = lambda x: (x-3)**2
res = opt.minimize(lambda v: quad(v[0]), x0=[0.0])
```

---

## Markowitz example (finance)

```python
# use returns from prices-top20.csv to estimate mu and Sigma
import pandas as pd, numpy as np
px = pd.read_csv('data/prices-top20.csv', parse_dates=['date']).sort_values(['symbol','date'])
ret = px.groupby('symbol')['close'].pct_change()
R = px.assign(ret=ret).pivot(index='date', columns='symbol', values='ret').dropna()
mu = R.mean().values
Sigma = np.cov(R.values, rowvar=False)

# min variance portfolio with sum(w)=1 and w>=0
N = len(mu)
var = lambda w: w @ Sigma @ w
cons = ({'type': 'eq', 'fun': lambda w: np.sum(w)-1}, {'type':'ineq','fun': lambda w: w})
res = opt.minimize(var, x0=np.ones(N)/N, constraints=cons, bounds=[(0,1)]*N)
weights = pd.Series(res.x, index=R.columns).sort_values(ascending=False)
weights.head()
```

---

## Minimization: constrained

```python
bounds = [(0, None)]
res = opt.minimize(lambda v: (v[0]-1)**2, x0=[2.0], bounds=bounds)
```

---

## Least squares

```python
import numpy as np
x = np.linspace(0, 1, 50)
y = 1 + 2*x + 0.1*np.random.randn(50)
A = np.c_[np.ones_like(x), x]
coef, *_ = np.linalg.lstsq(A, y, rcond=None)
```

---

## Gradient information

- Provide jac/hess when available
- Speeds convergence and improves accuracy

---

## Supplying Jacobian/Hessian

```python
def f(x): ...
def jac(x): ...
opt.minimize(f, x0, jac=jac, method='BFGS')
```

---

## Scaling and constraints

- Scale variables; use bounds/constraints
- Trust-constr for complex problems

---

## Bounds and linear constraints

```python
from scipy.optimize import Bounds, LinearConstraint
bounds = Bounds(0, 1)
A = [[1, 1]]; lb=[1]; ub=[1]
lin = LinearConstraint(A, lb, ub)
opt.minimize(var, x0, bounds=bounds, constraints=[lin])
```

---

## Diagnostics

- Check status, fun, grad, nfev
- Plot objective over iterations when possible

---

## Callbacks and termination

```python
def cb(xk): print(xk)
opt.minimize(f, x0, callback=cb, options={"maxiter": 200})
```

---

## Linear programming (HiGHS)

```python
from scipy.optimize import linprog
c = [1, 2]; A_ub=[[1,1]]; b_ub=[3]
linprog(c, A_ub=A_ub, b_ub=b_ub)
```

---

## Global optimization (basinhopping)

```python
from scipy.optimize import basinhopping
res = basinhopping(lambda x: (x[0]-3)**2 + 10*abs(np.sin(x[0])), x0=[0.0])
```

---

## Trust-constr highlights

- Handles bounds + (non)linear constraints with Jacobians
- Good default for complex constrained problems

---

## Debugging failures

- Inspect `res.message`, try different initial guesses
- Check gradients numerically; rescale variables

---

## Exercise 1

- Fit a logistic curve to data using least squares

---

## Exercise 2

- Minimize a portfolio variance under weight constraints on `prices-top20.csv`

---

<!-- removed by request: analogy content -->

---

## Wrap‑up

- You can solve roots and minimize objectives with SciPy and NumPy
- Next: Flask and Streamlit
