---
title: "Python Session 5 — Numerical Optimization"
subtitle: "Root finding, minimization, and solving equations"
author: "Juan F. Imbet"
institute: "Paris Dauphine University-PSL"
format:
  revealjs:
    theme: white
    css: styles.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Intro to VBA and Python"
    include-in-header: ../../tikzjax.html
execute:
  enabled: false
---

## Why optimize?

- Find best parameters or satisfy equations
- Analogy: finding the lowest valley in a landscape

---

## SciPy optimize

```python
from scipy import optimize as opt
```

---

## Root finding: scalar

```python
import math
from scipy import optimize as opt

f = lambda x: x*math.cos(x) - 1
root = opt.brentq(f, 0, 2)
```

---

## Root finding: systems

```python
import numpy as np

def F(v):
    x, y = v
    return [x**2 + y**2 - 1, x - y]
sol = opt.root(F, [0.5, 0.1])
```

---

## Minimization: unconstrained

```python
quad = lambda x: (x-3)**2
res = opt.minimize(lambda v: quad(v[0]), x0=[0.0])
```

---

## Markowitz example (finance)

```python
# use returns from prices-top20.csv to estimate mu and Sigma
import pandas as pd, numpy as np
px = pd.read_csv('data/prices-top20.csv', parse_dates=['date']).sort_values(['symbol','date'])
ret = px.groupby('symbol')['close'].pct_change()
R = px.assign(ret=ret).pivot(index='date', columns='symbol', values='ret').dropna()
mu = R.mean().values
Sigma = np.cov(R.values, rowvar=False)

# min variance portfolio with sum(w)=1 and w>=0
N = len(mu)
var = lambda w: w @ Sigma @ w
cons = ({'type': 'eq', 'fun': lambda w: np.sum(w)-1}, {'type':'ineq','fun': lambda w: w})
res = opt.minimize(var, x0=np.ones(N)/N, constraints=cons, bounds=[(0,1)]*N)
weights = pd.Series(res.x, index=R.columns).sort_values(ascending=False)
weights.head()
```

---

## Minimization: constrained

```python
bounds = [(0, None)]
res = opt.minimize(lambda v: (v[0]-1)**2, x0=[2.0], bounds=bounds)
```

---

## Least squares

```python
import numpy as np
x = np.linspace(0, 1, 50)
y = 1 + 2*x + 0.1*np.random.randn(50)
A = np.c_[np.ones_like(x), x]
coef, *_ = np.linalg.lstsq(A, y, rcond=None)
```

---

## Gradient information

- Provide jac/hess when available
- Speeds convergence and improves accuracy

---

## Scaling and constraints

- Scale variables; use bounds/constraints
- Trust-constr for complex problems

---

## Diagnostics

- Check status, fun, grad, nfev
- Plot objective over iterations when possible

---

## Exercise 1

- Fit a logistic curve to data using least squares

---

## Exercise 2

- Minimize a portfolio variance under weight constraints on `prices-top20.csv`

---

## Real‑life analogy

- Tuning a recipe: adjust ingredients to reach best taste

---

## Wrap‑up

- You can solve roots and minimize objectives with SciPy and NumPy
- Next: Flask and Streamlit
