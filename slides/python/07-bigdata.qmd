---
title: "Python Session 7 — Big Data with Polars and Dask"
subtitle: "Parallelism and lazy dataframes"
author: "Juan F. Imbet"
institute: "Paris Dauphine University-PSL"
format:
  revealjs:
    theme: white
    css: styles.css
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Intro to VBA and Python"
    include-in-header: ../../tikzjax.html
execute:
  enabled: false
---

## Why big data tools?

- Memory won’t hold everything; need out‑of‑core and parallelism


---

## Polars intro

```python
import polars as pl

df = pl.DataFrame({"a": [1,2,3], "b": [4,5,6]})
```

---

## Eager vs Lazy

```python
q = df.lazy().with_columns((pl.col("a") + pl.col("b")).alias("c"))
res = q.collect()
```

- Lazy builds a plan; executes once → fast

---

## Predicate pushdown

- Filters and projections applied early to reduce IO

---

## IO formats (finance dataset)

```python
# read course prices (long format) and compute vol per symbol
px = pl.read_csv('data/prices-top20.csv')
ret = px.sort(['symbol','date']).with_columns(
    pl.col('close').pct_change().over('symbol').alias('ret')
)
vol = ret.group_by('symbol').agg(pl.col('ret').std().alias('vol')).sort('vol', descending=True)
vol.head()
```

---

## Expressions and column ops

```python
df.with_columns((pl.col("a") * 2).alias("a2"))
```

---

## Groupby and joins in Polars

```python
df.groupby("a").agg(pl.col("b").sum())
```

---

## Polars window functions

```python
ret.with_columns(
  pl.col('ret').rolling_mean(window_size=21).over('symbol').alias('m21')
)
```

---

## Explain and inspect plan

```python
q = ret.lazy().group_by('symbol').agg(pl.col('ret').std())
print(q.explain())
```

---

## Streaming CSV read

```python
pl.scan_csv('big.csv').filter(pl.col('a')>0).select(['a','b']).collect()
```

---

## Dask intro (course dataset)

```python
import dask.dataframe as dd

df = dd.read_csv('data/prices-top20.csv', dtype={'symbol':'object'})
df['date'] = dd.to_datetime(df['date'])
df = df.set_index('date')
monthly = df.groupby([dd.Grouper(freq='M'), 'symbol'])['close'].last().pct_change()
monthly.head().compute()
```

- Partitioned DataFrame across cores/machines

---

## Dask compute graph

- Lazy; df.head() triggers small compute; df.compute() executes

---

## Map/Reduce style

- map_partitions, aggregations, groupby across partitions

---

## Scheduling and diagnostics

- Single‑machine threaded/process, distributed cluster
- Dashboard for monitoring tasks

---

## Partitions and persist

```python
df = df.repartition(npartitions=8).persist()
```

---

## Shuffles and groupby scale

- Wide groupby/joins require shuffles; watch cluster memory

---

## Memory tips

- Use categories, drop columns, write Parquet with snappy/zstd

---

## Parquet advantages

- Columnar, compressed, predicate pushdown; better than CSV at scale

---

## read_csv blocksize

```python
dd.read_csv('*.csv', blocksize='64MB')
```

---

## Schedulers

- threads, processes, distributed; choose per workload

---

## Diagnostics dashboard

- `client = Client()`; track tasks, memory, workers via dashboard

---

## When to use which?

- Polars: single‑node speed, expressive API
- Dask: scale out, interop with NumPy/pandas

---

## Exercise 1

- Convert `prices-top20.csv` to Parquet and compute per‑symbol volatility

---

## Exercise 2

- Use Dask to compute monthly returns by symbol and list the most volatile

---

<!-- removed by request: analogy content -->

---

## Wrap‑up

- You can process large datasets with Polars/Dask patterns
